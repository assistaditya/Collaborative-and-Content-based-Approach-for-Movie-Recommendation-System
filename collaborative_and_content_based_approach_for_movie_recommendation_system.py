# -*- coding: utf-8 -*-
"""Collaborative and Content based Approach for Movie Recommendation System.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Vf_5dMdnzSftaYnBt-dnNWCj-_ME3ZQI

**Collaborative and Content based Approach for Movie Recommendation System Source Code File**
"""

from google.colab import drive
drive.mount('/content/drive')

!unzip /content/drive/MyDrive/data.zip

"""**1. Import libraries**"""

!pip install scikit-surprise

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np
import ast
from scipy import stats
from ast import literal_eval
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.metrics.pairwise import linear_kernel, cosine_similarity
from nltk.stem.snowball import SnowballStemmer
from nltk.stem.wordnet import WordNetLemmatizer
from nltk.corpus import wordnet
from surprise import SVD, Reader
from surprise import Dataset
from surprise.model_selection import cross_validate

import warnings; warnings.simplefilter('ignore')

"""**2. Load dataset**"""

credits = pd.read_csv('credits.csv')
keywords = pd.read_csv('keywords.csv')
links_small = pd.read_csv('links_small.csv')
md = pd.read_csv('movies_metadata.csv')
ratings = pd.read_csv('ratings_small.csv')

"""**3. Understand dataset**"""

#Credits dataframe
credits.head()
#credits.iloc[0:3]
#credits['cast'].iloc[0:3]
#credits.iloc[:,0:2]

credits.columns

"""**cast**: Information about casting. Name of actor, gender and it's character name in movie.

**crew**: Information about crew members. Like who directed the movie, editor of the movie.

**id**: It's movie ID given by TMDb.
"""

credits.shape

credits.info()

"""**Keywords dataframe**"""

keywords.head()

keywords.columns

"""**id**: It's movie ID given by TMDb

**Keywords**: Tags/keywords for the movie. It list of tags/keywords
"""

keywords.shape

keywords.info()

"""**Link dataframe**"""

links_small.head()

links_small.columns

"""**movieId**: It's serial number for movie

**imdbId**: Movie id given on IMDb platform

**tmdbId**: Movie id given on TMDb platform
"""

links_small.shape

links_small.info()

"""**Metadata dataframe**"""

md.iloc[0:3].transpose()

md.columns

"""Features

**adult**: Indicates if the movie is X-Rated or Adult.

**belongs_to_collection**: A stringified dictionary that gives information on
 the movie series the particular film belongs to.

**budget**: The budget of the movie in dollars.

**genres**: A stringified list of dictionaries that list out all the genres associated with the movie.

**homepage**: The Official Homepage of the move.

**id**: The ID of the movie.

**imdb_id**: The IMDB ID of the movie.

**original_language**: The language in which the movie was originally shot in.

**original_title**: The original title of the movie.

**overview**: A brief blurb of the movie.

**popularity**: The Popularity Score assigned by TMDB.

**poster_path**: The URL of the poster image.

**production_companies**: A stringified list of production companies involved with the making of the movie.

**production_countries**: A stringified list of countries where the movie was shot/produced in.

**release_date**: Theatrical Release Date of the movie.

**revenue**: The total revenue of the movie in dollars.

**runtime**: The runtime of the movie in minutes.

**spoken_languages**: A stringified list of spoken languages in the film.

**status**: The status of the movie (Released, To Be Released, Announced, etc.)

**tagline**: The tagline of the movie.

**title**: The Official Title of the movie.

**video**: Indicates if there is a video present of the movie with TMDB.

**vote_average**: The average rating of the movie.

**vote_count**: The number of votes by users, as counted by TMDB.
"""

md.shape

md.info()

"""**Ratings dataframe**"""

ratings.head()

ratings.columns

"""**4. Pre-processing**
We will perform pre-processing as and when needed throughout
"""

md['genres'] = md['genres'].fillna('[]').apply(literal_eval).apply(lambda x: [i[
    'name'] for i in x] if isinstance(x, list) else [])

# this is V
vote_counts = md[md['vote_count'].notnull()]['vote_count'].astype('int')

# this is R
vote_averages = md[md['vote_average'].notnull()]['vote_average'].astype('int')

# this is C
C = vote_averages.mean()
C

m = vote_counts.quantile(0.95)
m

# Pre-processing step for getting year from date by splliting it using '-'

md['year'] = pd.to_datetime(md['release_date'], errors='coerce').apply(
    lambda x: str(x).split('-')[0] if x != np.nan else np.nan)

qualified = md[(md['vote_count'] >= m) &
               (md['vote_count'].notnull()) &
               (md['vote_average'].notnull())][['title',
                                                'year',
                                                'vote_count',
                                                'vote_average',
                                                'popularity',
                                                'genres']]

qualified['vote_count'] = qualified['vote_count'].astype('int')
qualified['vote_average'] = qualified['vote_average'].astype('int')
qualified.shape

def weighted_rating(x):
    v = x['vote_count']
    R = x['vote_average']
    return (v/(v+m) * R) + (m/(m+v) * C)

qualified['wr'] = qualified.apply(weighted_rating, axis=1)

qualified = qualified.sort_values('wr', ascending=False).head(250)

"""**Top Movies**"""

qualified.head(15)

s = md.apply(lambda x: pd.Series(x['genres']),axis=1).stack().reset_index(level=1, drop=True)
s.name = 'genre'
gen_md = md.drop('genres', axis=1).join(s)
gen_md.head(3).transpose()

def build_chart(genre, percentile=0.85):
    df = gen_md[gen_md['genre'] == genre]
    vote_counts = df[df['vote_count'].notnull()]['vote_count'].astype('int')
    vote_averages = df[df['vote_average'].notnull()]['vote_average'].astype('int')
    C = vote_averages.mean()
    m = vote_counts.quantile(percentile)

    qualified = df[(df['vote_count'] >= m) & (df['vote_count'].notnull()) &
                   (df['vote_average'].notnull())][['title', 'year', 'vote_count', 'vote_average', 'popularity']]
    qualified['vote_count'] = qualified['vote_count'].astype('int')
    qualified['vote_average'] = qualified['vote_average'].astype('int')

    qualified['wr'] = qualified.apply(lambda x:
                        (x['vote_count']/(x['vote_count']+m) * x['vote_average']) + (m/(m+x['vote_count']) * C),
                        axis=1)
    qualified = qualified.sort_values('wr', ascending=False).head(250)

    return qualified

"""**Top 15 Romantic Movies**"""

build_chart('Animation').head(15)

"""**5.1 Content based recommendation system**"""

links_small = links_small[links_small['tmdbId'].notnull()]['tmdbId'].astype('int')

## Pre-processing step

def convert_int(x):
    try:
        return int(x)
    except:
        return np.nan

md['id'] = md['id'].apply(convert_int)
md[md['id'].isnull()]

md = md.drop([19730, 29503, 35587])

md['id'] = md['id'].astype('int')

smd = md[md['id'].isin(links_small)]
smd.shape

"""We have 9099 movies available in our small movies metadata dataset which is 5 times smaller than our original dataset of 45000 movies.

**Content based recommendation system : Using movie description and taglines**

Let us first try to build a recommender using movie descriptions and taglines.

We do not have a quantitative metric to judge our machine's performance so this will have to be done qualitatively.
"""

smd['tagline'] = smd['tagline'].fillna('')
smd['description'] = smd['overview'] + smd['tagline']
smd['description'] = smd['description'].fillna('')

tf = TfidfVectorizer(analyzer='word',ngram_range=(1, 2),min_df=0.0, stop_words='english')
tfidf_matrix = tf.fit_transform(smd['description'])

tfidf_matrix.shape

"""we have used the TF-IDF Vectorizer, calculating the Dot Product will directly give us the Cosine Similarity Score.

Therefore, we will use sklearn's linear_kernel instead of cosine_similarities since it is much faster.
"""

# http://scikit-learn.org/stable/modules/metrics.html#linear-kernel
cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)

cosine_sim[0]
#cosine_sim.shape

"""We now have a pairwise cosine similarity matrix for all the movies in our dataset.

The next step is to write a function that returns the 30 most similar movies based on the cosine similarity score.
"""

smd = smd.reset_index()
titles = smd['title']
indices = pd.Series(smd.index, index=smd['title'])
#indices.head(2)

def get_recommendations(title):
    idx = indices[title]
    sim_scores = list(enumerate(cosine_sim[idx]))
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)
    sim_scores = sim_scores[1:31]
    movie_indices = [i[0] for i in sim_scores]
    return titles.iloc[movie_indices]

"""
* Let us now try and get the top recommendations for a few movies and see how good the recommendations are."""

get_recommendations('The Godfather').head(10)

get_recommendations('Superman II').head(10)

"""We see that for The Dark Knight, our system is able to identify it as a Batman film and subsequently recommend other Batman films as its top recommendations.


But unfortunately, that is all this system can do at the moment.


This is not of much use to most people as it doesn't take into considerations very important features such as cast, crew, director and genre, which determine the rating and the popularity of a movie.


Someone who liked The Dark Knight probably likes it more because of Nolan and would hate Batman Forever and every other substandard movie in the Batman Franchise.


Therefore, we are going to use much more suggestive metadata than Overview and Tagline.

In the next subsection, we will build a more sophisticated recommender that takes genre, keywords, cast and crew into consideration.

**Content based RS : Using movie description, taglines, keywords, cast, director and genres**

To build our standard metadata based content recommender, we will need to merge our current dataset with the crew and the keyword datasets.

Let us prepare this data as our first step.
"""

keywords['id'] = keywords['id'].astype('int')
credits['id'] = credits['id'].astype('int')
md['id'] = md['id'].astype('int')

md.shape

md = md.merge(credits, on='id')
md = md.merge(keywords, on='id')

smd = md[md['id'].isin(links_small)]
smd.shape

# smd = md[md['id'].isin(links_small['tmdbId'])]
# smd.shape

"""We now have our cast, crew, genres and credits, all in one dataframe. Let us wrangle this a little more using the following intuitions:


1. **Crew**: From the crew, we will only pick the director as our feature since the others don't contribute that much to the feel of the movie.


2. **Cast**: Choosing Cast is a little more tricky. Lesser known actors and minor roles do not really affect people's opinion of a movie. Therefore, we must only select the major characters and their respective actors. Arbitrarily we will choose the top 3 actors that appear in the credits list.
"""

smd['cast'] = smd['cast'].apply(literal_eval)
smd['crew'] = smd['crew'].apply(literal_eval)
smd['keywords'] = smd['keywords'].apply(literal_eval)
smd['cast_size'] = smd['cast'].apply(lambda x: len(x))
smd['crew_size'] = smd['crew'].apply(lambda x: len(x))

def get_director(x):
    for i in x:
        if i['job'] == 'Director':
            return i['name']
    return np.nan

smd['director'] = smd['crew'].apply(get_director)
smd['cast'] = smd['cast'].apply(lambda x: [i['name'] for i in x] if isinstance(x, list) else [])
smd['cast'] = smd['cast'].apply(lambda x: x[:3] if len(x) >=3 else x)
smd['keywords'] = smd['keywords'].apply(lambda x: [i['name'] for i in x] if isinstance(x, list) else [])

"""Approach to building the recommender is going to be extremely hacky.


What I plan on doing is creating a metadata dump for every movie which consists of genres, director, main actors and keywords.


I then use a Count Vectorizer to create our count matrix


The remaining steps are similar to what we did earlier: we calculate the cosine similarities and return movies that are most similar.


These are steps I follow in the preparation of my genres and credits data:


Strip Spaces and Convert to Lowercase from all our features. This way, our engine will not confuse between Johnny Depp and Johnny Galecki.

Mention Director 2 times to give it more weight relative to the entire cast.
"""

smd['cast'] = smd['cast'].apply(lambda x: [str.lower(i.replace(" ", "")) for i in x])
smd['director'] = smd['director'].astype('str').apply(lambda x: str.lower(x.replace(" ", "")))
smd['director'] = smd['director'].apply(lambda x: [x,x, x])

"""**Keywords**

We will do a small amount of pre-processing of our keywords before putting them to any use.

we calculate the frequenct counts of every keyword that appears in the dataset.
"""

s = smd.apply(lambda x: pd.Series(x['keywords']),axis=1).stack().reset_index(level=1, drop=True)
s.name = 'keyword'
s = s.value_counts()
s[:5]

"""Keywords occur in frequencies ranging from 1 to 610.

We do not have any use for keywords that occur only once.
Therefore, these can be safely removed.

Finally, we will convert every word to its stem so that words such as Dogs and Dog are considered the same.
"""

s = s[s > 1]

# Just an example
stemmer = SnowballStemmer('english')
stemmer.stem('dogs')

def filter_keywords(x):
    words = []
    for i in x:
        if i in s:
            words.append(i)
    return words

smd['keywords'] = smd['keywords'].apply(filter_keywords)
smd['keywords'] = smd['keywords'].apply(lambda x: [stemmer.stem(i) for i in x])
smd['keywords'] = smd['keywords'].apply(lambda x: [str.lower(i.replace(" ", "")) for i in x])

smd['soup'] = smd['keywords'] + smd['cast'] + smd['director'] + smd['genres']
smd['soup'] = smd['soup'].apply(lambda x: ' '.join(x))

count = CountVectorizer(analyzer='word',ngram_range=(1, 2),min_df=0.0, stop_words='english')
count_matrix = count.fit_transform(smd['soup'])

cosine_sim = cosine_similarity(count_matrix, count_matrix)

smd = smd.reset_index()
titles = smd['title']
indices = pd.Series(smd.index, index=smd['title'])

"""We will reuse the get_recommendations function that we had written earlier.

Since our cosine similarity scores have changed, we expect it to give us different (and probably better) results.

Let us check for The Dark Knight again and see what recommendations I get this time around.
"""

get_recommendations('The Prestige').head(10)

"""I am much more satisfied with the results I get this time around.

The recommendations seem to have recognized other Christopher Nolan movies (due to the high weightage given to director) and put them as top recommendations.

I enjoyed watching The Dark Knight as well as some of the other ones in the list including Batman Begins, The Prestige and The Dark Knight Rises.

**Improvment**

We can of course experiment on this engine by trying out different weights for our features (directors, actors, genres), limiting the number of keywords that can be used in the soup, weighing genres based on their frequency, only showing movies with the same languages, etc.

get_recommendations('Insomnia').head(10)
"""

get_recommendations('The DUFF').head(10)

get_recommendations('Pulp Fiction').head(10)

"""Add Popularity and Ratings
One thing that we notice about our recommendation system is that it recommends movies regardless of ratings and popularity. It is true that Batman and Robin has a lot of similar characters as compared to The Dark Knight but
it was a terrible movie that shouldn't be recommended to anyone.

Therefore, we will add a mechanism to remove bad movies and return movies which are popular and have had a good critical response.

I will take the top 25 movies based on similarity scores and calculate the vote of the 60th percentile movie. Then, using this as the value of  m , we will calculate the weighted rating of each movie using IMDB's formula like we did in the Simple Recommender section.
"""

def improved_recommendations(title):
    idx = indices[title]
    sim_scores = list(enumerate(cosine_sim[idx]))
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)
    sim_scores = sim_scores[1:26]
    movie_indices = [i[0] for i in sim_scores]

    movies = smd.iloc[movie_indices][['title', 'vote_count', 'vote_average', 'year']]
    vote_counts = movies[movies['vote_count'].notnull()]['vote_count'].astype('int')
    vote_averages = movies[movies['vote_average'].notnull()]['vote_average'].astype('int')
    C = vote_averages.mean()
    m = vote_counts.quantile(0.60)
    qualified = movies[(movies['vote_count'] >= m) & (movies['vote_count'].notnull()) &
                       (movies['vote_average'].notnull())]
    qualified['vote_count'] = qualified['vote_count'].astype('int')
    qualified['vote_average'] = qualified['vote_average'].astype('int')
    qualified['wr'] = qualified.apply(weighted_rating, axis=1)
    qualified = qualified.sort_values('wr', ascending=False).head(10)
    return qualified

improved_recommendations('Batman Begins')

improved_recommendations('Jackie Brown')

"""Unfortunately, Batman and Robin does not disappear from our recommendation list.

This is probably due to the fact that it is rated a 4, which is only slightly below average on TMDB.

It certainly doesn't deserve a 4 when amazing movies like The Dark Knight Rises has only a 7.

However, there is nothing much we can do about this. Therefore, we will conclude our Content Based Recommender section here

**CF based recommendation system**

**Our content based engine suffers from some severe limitations.**

It is only capable of suggesting movies which are close to a certain movie. That is, it is not capable of capturing tastes and providing recommendations across genres.

Also, the engine that we built is not really personal in that it doesn't capture the personal tastes and biases of a user. Anyone querying our engine for recommendations based on a movie will receive the same recommendations for that movie, regardless of who (s)he is.

Therefore, in this section, we will use Collaborative Filtering to make recommendations to Movie Watchers. Collaborative Filtering is based on the idea that users similar to a me can be used to predict how much I will like a particular product or service those users have used/experienced but I have not.

I will not be implementing Collaborative Filtering from scratch. Instead, I will use the Surprise library that used extremely powerful algorithms like Singular Value Decomposition (SVD) to minimise RMSE (Root Mean Square Error) and give great recommendations.
"""

# surprise reader API to read the dataset
!pip install surprise
from surprise import Reader
reader = Reader()

from surprise.model_selection import train_test_split

# Load your data as a DatasetAutoFolds object
data = Dataset.load_from_df(ratings[['userId', 'movieId', 'rating']], reader)

# Split the data into a trainset and a testset
trainset, testset = train_test_split(data, test_size=0.2)  # You can adjust the test_size as needed

from surprise import SVD
from surprise import Dataset
from surprise import Reader
from surprise.model_selection import cross_validate

# Define the Reader object
reader = Reader(rating_scale=(1, 5))

# Load your data as a Dataset object
data = Dataset.load_from_df(ratings[['userId', 'movieId', 'rating']], reader)

# Initialize the SVD algorithm
svd = SVD()

# Perform cross-validation
results = cross_validate(svd, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)

# Print the results
for measure in ['test_rmse', 'test_mae']:
    print(f'{measure}: {results[measure].mean()}')

ratings[ratings['userId'] == 2]

svd.predict(1, 302)

"""For movie with ID 302, we get an estimated prediction of 2.686. One startling feature of this recommender system is that it doesn't care what the movie is (or what it contains). It works purely on the basis of an assigned movie ID and tries to predict ratings based on how the other users have perceive the movie.

**Hybrid recommendation system**

In this section, will try to build a simple hybrid recommender that brings together techniques we have implemented in the content based and collaborative filter based engines. This is how it will work:

Input: User ID and the Title of a Movie

Output: Similar movies sorted on the basis of expected ratings by that particular user.
"""

def convert_int(x):
    try:
        return int(x)
    except:
        return np.nan

id_map = pd.read_csv('links_small.csv')[['movieId', 'tmdbId']]
id_map['tmdbId'] = id_map['tmdbId'].apply(convert_int)
id_map.columns = ['movieId', 'id']
id_map = id_map.merge(smd[['title', 'id']], on='id').set_index('title')
#id_map = id_map.set_index('tmdbId')

indices_map = id_map.set_index('id')

def hybrid(userId, title):
    idx = indices[title]
    tmdbId = id_map.loc[title]['id']
    movie_id = id_map.loc[title]['movieId']
    sim_scores = list(enumerate(cosine_sim[int(idx)]))
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)
    sim_scores = sim_scores[1:26]
    movie_indices = [i[0] for i in sim_scores]
    movies = smd.iloc[movie_indices][['title', 'vote_count', 'vote_average', 'release_date', 'id']]
    movies['est'] = movies['id'].apply(lambda x: svd.predict(userId, indices_map.loc[x]['movieId']).est)
    movies = movies.sort_values('est', ascending=False)
    return movies.head(10)

hybrid(1, 'Avatar')

hybrid(1000, 'Replicant')

hybrid(3423, "The Terminator")